---
layout: archive
title: "Featured Research"
permalink: /blog/
author_profile: true
---

{% include base_path %}

---

## What We Know and What We Don’t Know About Combatting Misinformation
<figure style="text-align: center; margin-top: 1em; margin-bottom: 1em;">
  <img src="/assets/images/articles_category.png" alt="Interventions Studied" style="width:70%; height:auto;">
  <figcaption style="font-size: 0.9em; color: #555; margin-top: 0.5em;">
    Intervention categories studied in misinformation research over the past 20 years.
  </figcaption>
</figure>
Over the last twenty years, researchers have proposed many strategies to combat misinformation, including fact-checking, media literacy campaigns, and warning labels. But which interventions get studied the most, and which ones actually work? We conducted a bibliometric review of over 450 papers on misinformation strategies at the user, platform, and policy levels. Our main findings: 
<ol>
  <li>
    <b>Most research is on the same few interventions</b><br>
   Most research focuses on fact-checking, debunking, and media literacy, while other promising strategies, such as redirecting users, account demonetization, or improving user reporting tools, remain largely overlooked. 
  </li>
  <li>
    <b>The field remains relatively siloed, although cross-disciplinary collaboration is growing.</b><br>
    Research on misinformation spans multiple disciplines, encompassing fields such as communications, psychology, computer science, and political science. However, authors still mostly publish in journals within their primary disciplines. 
  </li>
  <li>
    <b>Most papers study effectiveness without considering user acceptance.</b><br>
    Nearly 90% of papers assess the effectiveness of interventions, but only 9% study user acceptance. This suggests we may know which strategies could work in theory, but we don’t know if people are willing to adopt them. Without user buy-in, platforms are unlikely to implement these interventions.
  </li>
  <li>
    <b>Some findings on intervention effectiveness are inconsistent</b><br>
    Even among the most researched interventions, results vary. For example, some studies show inoculation games (e.g., Bad News) are effective, but others have found minimal or no long-term effects. This highlights the need for more rigorous, standardized evaluation methods.
  </li>
</ol>
If we want to counter the spread of misinformation, we need solutions that are not only effective in lab settings but also accepted, usable, and scalable in real-world environments. Future work should focus on interdisciplinary collaboration, investigating understudied intervention types, and assessing interventions not only based on effectiveness but also on user acceptance.

[Full paper](https://workshop-proceedings.icwsm.org/pdf/2025_10.pdf)<br>
keywords: misinformation interventions, scoping review, bibliometrics

<i>2025-07-25</i>

---

## Understanding User Behavior in the Fight Against Social Media Misinformation
<img src="/assets/images/sci-reports-blog.png" alt="Correcting Misinformation" style="width:60%; height:auto; margin-top:1em; margin-bottom:1em;">
As social media companies continue fighting misinformation, research often focuses on platform interventions like fact-checking and moderation. However, little is known about how users respond to misinformation in feeds. Do they ignore, report, or correct it? Understanding these reactions is important because they influence the spread of false content. This study surveyed over 1,000 active American social media users, examining their beliefs, actions, and how their relationships with misinformation posters impact their decisions. We identified three main findings:
<ol>
  <li>
    <b>There's a large gap in beliefs vs. actions</b><br>
    One of our key findings was the evidence of hypocrisy between what participants believe and what they do. Respondents thought others should put more effort into correcting misinformation than they claimed to do themselves. This gap shows that although many people value combatting misinformation, they don't always follow through, possibly assuming others will. It also suggests that reducing barriers like time or confidence could encourage more active countering on social media. 
  </li>
  <li>
    <b>People say they are more Likely to counter closer contacts</b><br>
    We observed a social proximity effect: people are more likely to intervene when misinformation comes from friends or family than acquaintances or strangers. This may be because they feel more comfortable confronting close contacts or have a stronger sense of responsibility to act when close contacts spread misinformation.
  </li>
  <li>
    <b>There is widespread support for user-driven interventions across the political spectrum</b><br>
    User-led interventions received broad political support, unlike stricter measures like content moderation or government regulation. Over 70% of Americans across parties believe people should actively counter misinformation from close contacts. It's promising that both strong Republicans and Democrats hold this view amid high polarization. 
  </li>
</ol>

[Full paper](https://www.nature.com/articles/s41598-025-93100-7), [Full blog](https://www.cmu.edu/ideas-social-cybersecurity/news1/blog-posts/blog-king-understanding-user.html)<br>
keywords: misinformation interventions, social media misinformation, user behavior, survey data

<i>2025-04-18</i>

---