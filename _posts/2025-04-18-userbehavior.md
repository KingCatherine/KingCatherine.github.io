---
title: 'Understanding User Behavior in the Fight Against Social Media Misinformation'
date: 2025-04-18
permalink: /posts/2025/userbehavior/
layout: single
tags:
  - misinformation interventions
  - social media misinformation
  - user behavior
  - survey data
excerpt: >-
  <img src="/assets/images/sci-reports-blog.png" alt="Correcting Misinformation"
       style="width:60%;height:auto;margin-bottom:0.6em;">
  <br>
  Surveying over 1,000 U.S. social media users, we examine how beliefs and relationships
  shape whether people ignore, report, or correct misinformation. The results reveal a gap
  between values and actions.
---

<img src="/assets/images/sci-reports-blog.png" alt="Correcting Misinformation" style="width:60%; height:auto; margin-top:1em; margin-bottom:1em;">

As social media companies continue fighting misinformation, research often focuses on platform interventions like fact-checking and moderation. However, little is known about how users respond to misinformation in feeds. Do they ignore, report, or correct it? Understanding these reactions is important because they influence the spread of false content. This study surveyed over 1,000 active American social media users, examining their beliefs, actions, and how their relationships with misinformation posters impact their decisions. 

<!-- more -->

We identified three main findings:
<ol>
  <li>
    <b>There's a large gap in beliefs vs. actions</b><br>
    One of our key findings was the evidence of hypocrisy between what participants believe and what they do. Respondents thought others should put more effort into correcting misinformation than they claimed to do themselves. This gap shows that although many people value combatting misinformation, they don't always follow through, possibly assuming others will. It also suggests that reducing barriers like time or confidence could encourage more active countering on social media. 
  </li>
  <li>
    <b>People say they are more Likely to counter closer contacts</b><br>
    We observed a social proximity effect: people are more likely to intervene when misinformation comes from friends or family than acquaintances or strangers. This may be because they feel more comfortable confronting close contacts or have a stronger sense of responsibility to act when close contacts spread misinformation.
  </li>
  <li>
    <b>There is widespread support for user-driven interventions across the political spectrum</b><br>
    User-led interventions received broad political support, unlike stricter measures like content moderation or government regulation. Over 70% of Americans across parties believe people should actively counter misinformation from close contacts. It's promising that both strong Republicans and Democrats hold this view amid high polarization. 
  </li>
</ol>

[Full paper](https://www.nature.com/articles/s41598-025-93100-7)

[CASOS blog](https://www.cmu.edu/ideas-social-cybersecurity/news1/blog-posts/blog-king-understanding-user.html)<br>
